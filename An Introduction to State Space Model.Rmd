---
title: "An Introduction to State Space Model"
output: 
  pdf_document:
     toc: true
     toc_depth: 3
     fig_width: 4.5
     fig_height: 3
     fig_caption: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Basic Idea
Denote $s_t$ as a state variable, which cannot be observed, and $y_t$ as the measurement of $s_t$, which is observable. denote $Y_t$ as the sequence $y_1,y_2,,,,y_t$. A state space model is given by:

- The probability of $s_t$ given the $s_{t-1}$, and $Y_{t-1}$
$$M(s_t | s_{t-1},Y_{t-1},\theta) \tag{1}$$

- The probability of $y_t$ given the $s_t$ and $Y_{t-1}$.
$$g(y_t | s_t,Y_{t-1},\gamma ) \tag{2}$$

In which $\theta$ and $\gamma$ are parameters. We we want to estimate $\theta$ and $\gamma$.But how?  The most used method is still MLE. Since we can only observe $y_1,y_2,....y_T$, the likelihood function for the total sample is
$$f(y_1,y_2,...y_T)=f_y(y_1)f_y(y_2|y_1)f_y(y_3|y_2,y_1)...=f_y(y_1)\prod_{2}^{T}{f_y(y_t | Y_{t-1})}$$
Here $f_y$ is the probability distribution of $y$. Therefore we need to find out the expression for $f_y(y_t | Y_{t-1})$. In fact this is easy to process.According to definition, we have
$$f_y(y_t |Y_{t-1})= \int{g(y_t |s_t,Y_{t-1})f_s(s_t|Y_{t-1})}ds_t \tag{3} $$
Here $f_s$ denotes the probability distribution function of $s$.$g(y_t |s_t,Y_{t-1})$is given, but $f_s(s_t|Y_{t-1})$ is still unknown. But we have 
$$f_s(s_t|Y_{t-1})=\int{M(s_t|s_{t-1},Y_{t-1})f_s(s_{t-1}|Y_{t-1})}ds_{t-1} \tag{4}$$
Therefore, from (3) and (4) we know that we only need to know $f_s(s_{t-1}|Y_{t-1})$ if we want to know $f_y(y_t |Y_{t-1})$. (There is nothing new here in the derivation, only a combination of conditional probability).
<p> Our next step is to get the expression of $f_s(s_{t-1}| Y_{t-1})$, but we can first see $f(s_{t}| Y_{t})$. Logically, $Y_t$ is determined by $s_t$, therefore what we are looking at is like a posterior 'belief' on $s_t$ after we observe $Y_t$. Therefore we use Bayesian formula and get:
    $$f_s(s_{t}| Y_{t})=\frac{f(s_t,Y_t)}{f(Y_t)}=\frac{f(s_t,y_t,Y_{t-1})}{f(y_t,Y_{t-1})}=\frac{g(y_t|s_t,Y_{t-1})f(s_t,Y_{t-1})}{f_y(y_t|Y_{t-1})f(Y_{t-1})}=\frac{g(y_t|s_t,Y_{t-1})f_s(s_t|Y_{t-1})}{f_y(y_t|Y_{t-1})} \tag{5}$$
    Combining (3)(4)(5) :if we know $f_s(s_{t-1}|Y_{t-1})$, we can get $f_s(s_{t}| Y_{t})$. Therefore, starting from a given $f_s(s_0|Y_0)$ we can use derive the whole likelihood function recursively according to (3)(4)(5).</p>

- use $f_s(s_{t-1}| Y_{t-1})$ to get$f_s(s_t|Y_{t-1})$ and $f_y(y_t |Y_{t-1})$  by (3) and (4)
- use (5) to get $f_s(s_{t}| Y_{t})$

Through the whole process we get the whole series of expression for $f_y(y_t |Y_{t-1})$.If all the variables are discrete, the calculation may be easy; when the variables are continuous, however, (3) and (4) requires integral which is often difficult, except for the case where the variables are generated from normal distribution. Above process is call Kalma Filter when we assume normal distribution. 

# Kalma Filter & Smoother

We assume that the state space model now looks like
$$s_t = Ts_{t-1}+R\eta_t,\tag{6}$$
$$y_t= Zs_t +S\xi_t \tag{7}$$
in which $\eta_t$ and $\xi_t$ are independent and $\eta_t \sim N(0,Q)$ and $\xi_t  \sim N(0,H)$. We repeat the procedure in the previous section to get the estimation $T,R,Z,S,Q,H$. The procedure is called kalma filter.


## Kalma Filter
. Following the steps in the basic idea,we want to know the distribution of the following three items:
$$s_t|Y_{t-1}, y_t|Y_{t-1}, \rightarrow s_t | Y_t$$.
A good point on the normal distribution is that, if we assume $s_0$ follows a normal distribution, then all the $y_t, Y_t, s_t$  also follow normal distribution, and $s_t|Y_{t-1}, y_t|Y_{t-1},  s_t | Y_t$ also follows normal distributions. Denote
$$s_t|Y_{t-1}\sim N(\alpha_{t|t-1},P_{t|t-1}), y_t|Y_{t-1} \sim N(\mu_{t|t-1},F_t), s_t | Y_t \sim N(\alpha_{t|t},P_{t|t})$$
The last one implies that $s_{t-1} | Y_{t-1} \sim N(\alpha_{t-1|t-1},P_{t-1|t-1})$.Apparently, given the information on $Y_{t-1}$, we have according to (6) that 
$$\alpha_{t|t-1}=T \alpha_{t-1|t-1},\tag{8}$$
$$P_{t|t-1}=T P_{t-1|t-1}T'+RQR',\tag{9}$$
We have according to (7) that 
$$\mu_{t|t-1}=Z \alpha_{t|t-1}=ZT \alpha_{t-1|t-1},\tag{10}$$
$$F_t = Z P_{t|t-1}Z' + SHS',\tag{11}$$
Therefore, starting from the distribution of $s_{t-1}|Y_{t-1}$, which is characterized by $\alpha_{t-1|t-1}$ and $P_{t-1|t-1}$, we can get distribution of $s_{t-1}|Y_{t-1}$, which is characterized by $\alpha_{t|t-1}$ and $P_{t|t-1}$, and the distribution of $y_{t-1}|Y_{t-1}$, which is characterized by $\mu_{t|t-1}$ and $F_t$.
But again here is a question: How should we get $\alpha_{t-1|t-1}$ and $P_{t-1|t-1}$?
<p> Again, we approach this problem by checking the distribution of $s_{t}|Y_{t}$, which provides information on $\alpha_{t|t}$ and $P_{t|t}$. which is equivalent to checking the distribution of $s_{t}|y_{t}$ given the $Y_{t-1}$. Since both $s_t$ and $y_t$ follows normal distribution, statistics textbook tells us that the joint distribution of $s_t$ and $y_t$ is also normal, and we can calculate the distribution of $s_{t}|y_{t}$ given the their joint distribution. </p>
<p> Suppose we already know the mean and variance of the $s_t$ and $y_t$ respectively, given $Y_{t-1}$. We can immediately write down the joint distribution of $s_t$ and $y_t$ if we know $cov(s_t,y_t)$, and this is easy. Notice that (given $Y_{t-1}$)
    $$cov(s_t,y_t)=cov(s_t, Zs_t +S\xi_t \tag{7})=var(s_t)Z'=P_{t|t-1}Z'$$
    Textbook immediately tells us that us 
    $$s_t |y_t \sim N\left(\alpha_{t|t-1}+P_{t|t-1}Z'F^{-1}_t(y_t-\mu_{t|t-1})),P_{t|t-1}+P_{t|t-1}Z'F^{-1}_tZP_{t|t-1}\right)$$
    which implies that
    $$\alpha_{t|t}=\alpha_{t|t-1}+P_{t|t-1}Z'F^{-1}_t(y_t-\mu_{t|t-1})),P_{t|t}=P_{t|t-1}+P_{t|t-1}Z'F^{-1}_tZP_{t|t-1} \tag{12}$$
    Now the procedure is very clear: </p>
    
- given the distribution of $s_{t-1}| y_{t-1}$, i.e., $\alpha_{t-1|t-1}$ and $P_{t-1|t-1}$, we get the $\alpha_{t|t-1}$ by (8), get the $P_{t|t-1}$ by (9), get the $\mu_{t|t-1}$ by (10), and get the $F_t$ by (11).
- get the $\alpha_{t|t}$ and $P_{t|t}$ by (12)

An easier way to understand the above process is as follows:
1. suppose we already know $\eta_1$ and $s_0$.
2. then we know the value of $s_1$. Since we have the data for $y_1$, we also know $\xi_1$.
3. Since $s_2 = Ts_1 + R \eta_2$, $y_2 = Zs_2 + S\xi_2$ we immediately have 
$$y_2 = Z (Ts_1 +R \eta_2)+ S\xi_2$$
which means we know that 
$$Z R \eta_2 + S \xi_2 = y_2 - ZTs_1= y_2 - Z T(Z^{-1}y_1-Z^{-1}S \xi_1)$$
therefore
$$Z R \eta_2 + S \xi_2 - Z T Z^{-1}S \xi_1 = y_2 - Z TZ^{-1}y_1$$
similarly we have 
$$Z R \eta_t + S \xi_t - Z T Z^{-1}S \xi_{t-1} = y_t - Z TZ^{-1}y_{t-1}$$
Denote $\theta_t \equiv Z R \eta_t + S \xi_t - Z T Z^{-1}S \xi_{t-1}$. Intuitively, the likelihood function is 
$$pr(\theta_T = y_T - Z TZ^{-1}y_{T-1}, \theta_{T-1}= y_{T-1} - Z TZ^{-1}y_{T-2}...,\theta_1 =  y_1 - Z TZ^{-1}y_0)$$
$$=pr(\theta_1 =  y_1 - Z TZ^{-1}y_0)pr(\theta_2 =  y_2 - Z TZ^{-1}y_1| \theta_1 =  y_1 - Z TZ^{-1}y_0)pr(\theta_3 =  y_3 - Z TZ^{-1}y_2 |\theta_2 =  y_2 - Z TZ^{-1}y_1).....$$
Therefore we need to know the distribution of $\theta_t | \theta_{t-1}$.This is not difficult if we can get the joint distribution of $\theta_t$ and $\theta_{t-1}$. First, both of them are normal distributed with mean as 0 and their variance are easy to calculate, since both are just linear combination of independent normally distributed disturbance $\xi$ and $\eta$. Second, 
$$cov(\theta_t, \theta_{t-1})= {\left( ZTZ^{-1} \right)}^2S^2 H$$
Therefore we can write down the joint distribution of $\theta_t$ and $\theta_{t-1}$ and we can also get the distribution of $\theta_t | \theta_{t-1}$.

## Karma Smoother 

Sometimes, we want to estimate the $s_t$ given the whole observed series $Y_T$. i.e., we want to see 
$$E(s_t| Y_T) \equiv \beta_{t|T}$$
To address this, first notice that $s_t$ is directly linked to $s_{t+1}$ and $Y_t$. Intuitively, if the information of $s_{t+1}$ is given, we actually do not need the information on $y_{t+1}$ and so on ( <b> the reason is that, given $s_{t+1}$, $y_{t+1}$ and on are independent of $s_t$: given $s_{t+1}$, the variance of $y_{t+1}$ is caused by $\xi_{t+1}$, while the variance of $s_t$ is given by $\eta_t$. $\eta_t$ and $\xi_{t+1}$ are independent. </b>). Therefore our key task is to get 
$$E(s_t | Y_t, s_{t+1})$$
This is easy if we know the joint distribution of $s_t$ and $s_{t+1}$ given the $Y_t$. Recall that 
$$s_t | Y_t \sim N(\alpha_{t|t},P_{t|t}),s_{t+1} | Y_t \sim N(\alpha_{t+1|t},P_{t+1|t})$$
and we have 
$$cov(s_t,s_{t+1})=Tvar(s_t)=TP_{t|t}$$
Therefore, again, we have 
$$E(s_t |s_{t+1},Y_t)=\alpha_{t|t}+TP_{t|t}P^{-1}_{t+1|t}(s_{t+1}-\alpha_{t+1|t})$$
since, given $s_{t+1}$ and $Y_t$, the distribution of $s_t$ is independent of $y_{t'}$ for $t'>t$, we have
$$E(s_t |s_{t+1},Y_T)=E(s_t |s_{t+1},Y_t)=\alpha_{t|t}+TP_{t|t}P^{-1}_{t+1|t}(s_{t+1}-\alpha_{t+1|t})$$
But what we need is $E(s_t |Y_t)$, there we take expectation of $E(s_t |s_{t+1},Y_T)$ w.r.t $s_{t+1}$, and have
$$\beta_{t|T}\equiv E(s_t |Y_T)=\alpha_{t|t}+TP_{t|t}P^{-1}_{t+1|t}(E(s_{t+1}|Y_T)-\alpha_{t+1|t})=\alpha_{t|t}+TP_{t|t}P^{-1}_{t+1|t}( \beta_{t+1|T}-\alpha_{t+1|t})$$
Therefore we can calculate the $\beta_{t|T}$ recursively.

# Example: Local Level Model
$$y_t = s_t + \epsilon_t$$
$$s_t = \beta s_{t-1} + u_t$$
in which $s_t$ is unobservable. $\epsilon$ and $u_t$ are independent error term.
$$\epsilon \sim  N(0, \sigma^2_{\epsilon}), u \sim N(0, \sigma^2_{u}) $$

It is easy to know that 
$$(y_t - \epsilon_t) = \beta (y_{t-1}- \epsilon_{t-1})+ u_t$$
which can be rewritten as
$$\epsilon_t - \beta \epsilon_{t-1} + u_t = y_t - \beta y_{t-1}$$
treat $s_0$ as a parameter that we want to estimate. We can write down the likelihood function as 
$$pr(\epsilon_0 = y_0 - s_0 ,\epsilon_1 - \beta \epsilon_{0} + u_1 = y_1 - \beta y_{0}, \epsilon_2 - \beta \epsilon_{1} + u_2 = y_2 - \beta y_{1},... )$$
$$=pr( \epsilon_0 = y_0 - s_0) pr(\epsilon_1 - \beta \epsilon_{0} + u_1 = y_1 - \beta y_{0} | \epsilon_0 = y_0 - s_0)pr( \epsilon_2 - \beta \epsilon_{1} + u_2 = y_2 - \beta y_{1}|\epsilon_1 - \beta \epsilon_{0} + u_1 = y_1 - \beta y_{0})...$$

first we have 
$$pr(\epsilon_1 - \beta \epsilon_{0} + u_1 = y_1 - \beta y_{0} | \epsilon_0 = y_0 - s_0) = pr(\epsilon_1  + u_1 = y_1 - \beta y_{0}+ \beta (y_0-s_0) )$$
This probability is easy to compute, since we know that $\epsilon_1  + u_1 \sim N(0, \sigma_\epsilon^2 + \sigma_u^2)$.

Next, denote 
$$\theta_t \equiv \epsilon_t - \beta \epsilon_{t-1} + u_t$$
we easily have 
$$\theta_t \sim N(0, (1+\beta^2)\sigma^2_{\epsilon} + \sigma^2_u), \quad \forall t \ge1$$
and 
$$\rho(\theta_t,\theta_{t-1}) = \frac{cov(\theta_t,\theta_{t-1})}{\sqrt{var(\theta_t)var(\theta_{t-1})}} = \frac{-\beta \sigma_{\epsilon}^2}{((1+\beta^2)\sigma^2_{\epsilon} + \sigma^2_u)}$$
Therefore we have 
$$\theta_t | \theta_{t-1} = y_{t-1} - \beta y_{t-2} \sim N\left( \rho( y_{t-1} - \beta y_{t-2}), (1-\rho^2) \left((1+\beta^2)\sigma^2_{\epsilon} + \sigma^2_u\right)\right ) $$
